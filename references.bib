
@article{ko_practical_2015,
	title = {A practical guide to controlled experiments of software engineering tools with human participants},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9279-3},
	doi = {10.1007/s10664-013-9279-3},
	abstract = {Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.},
	pages = {110--141},
	number = {1},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Ko, Andrew J. and {LaToza}, Thomas D. and Burnett, Margaret M.},
	urldate = {2019-10-23},
	date = {2015-02},
	langid = {english}
}

@article{amirteimoori_production_2013,
	title = {Production planning in data envelopment analysis without explicit inputs},
	volume = {47},
	issn = {0399-0559, 1290-3868},
	url = {http://www.rairo-ro.org/10.1051/ro/2013038},
	doi = {10.1051/ro/2013038},
	abstract = {In the performance measurement using tools such as data envelopment analysis ({DEA}), data without explicit inputs has attracted considerable attention among researchers. In such studies the problem of production planning in the next production season is an important and interesting subject. Because of the uncertain nature of the future, decision makers need to provide robust procedures in order to examine alternative courses of action and their implications. The purpose of this paper is to develop an approach to production planning problem in production processes without explicit inputs that typically appears in centralized decision making environment. Application of the proposed approach is illustrated empirically using a real case.},
	pages = {273--284},
	number = {3},
	journaltitle = {{RAIRO} - Operations Research},
	shortjournal = {{RAIRO}-Oper. Res.},
	author = {Amirteimoori, Alireza and Daneshian, Behrooz and Kordrostami, Sohrab and Shahroodi, Kambiz},
	urldate = {2019-10-23},
	date = {2013-07},
	langid = {english},
	keywords = {{DEA}-{WEI}, read}
}

@article{liu_study_2011,
	title = {A study of {DEA} models without explicit inputs},
	volume = {39},
	issn = {03050483},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305048310001295},
	doi = {10.1016/j.omega.2010.10.005},
	abstract = {In performance evaluations, data without explicit inputs (such as index data, pure output data) are widely used. To directly use such data, this paper presents a study on building {DEA} models without explicit inputs, so-called {DEA}-{WEI} models, which are applicable to the evaluation applications where inputs are not directly considered. We provide an axiom foundation of these kinds of models, and further discuss how to incorporate value judgments of decision makers into these {DEA}-{WEI} models. Several such models are derived. Finally, applications of the {DEA}-{WEI} models are presented.},
	pages = {472--480},
	number = {5},
	journaltitle = {Omega},
	shortjournal = {Omega},
	author = {Liu, W.B. and Zhang, D.Q. and Meng, W. and Li, X.X. and Xu, F.},
	urldate = {2019-10-23},
	date = {2011-10},
	langid = {english}
}

@article{yang_extended_2014,
	title = {Extended utility and {DEA} models without explicit input},
	volume = {65},
	issn = {0160-5682, 1476-9360},
	url = {https://www.tandfonline.com/doi/full/10.1057/jors.2013.68},
	doi = {10.1057/jors.2013.68},
	pages = {1212--1220},
	number = {8},
	journaltitle = {Journal of the Operational Research Society},
	shortjournal = {Journal of the Operational Research Society},
	author = {Yang, Guoliang and Shen, Wanfang and Zhang, Daqun and Liu, Wenbin},
	urldate = {2019-10-21},
	date = {2014-08},
	langid = {english}
}

@article{kitchenham_systematic_2009,
	title = {Systematic literature reviews in software engineering – A systematic literature review},
	abstract = {Background: In 2004 the concept of evidence-based software engineering ({EBSE}) was introduced at the {ICSE}04 conference. Aims: This study assesses the impact of systematic literature reviews ({SLRs}) which are the recommended {EBSE} method for aggregating evidence.
Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven {SLRs} addressed cost estimation. The quality of {SLRs} was fair with only three scoring less than 2 out of 4.
Conclusions: Currently, the topic areas covered by {SLRs} are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation {SLRs} demonstrate the potential value of {EBSE} for synthesising evidence and making it available to practitioners.},
	pages = {9},
	journaltitle = {Information and Software Technology},
	author = {Kitchenham, Barbara and Brereton, O Pearl and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	date = {2009},
	langid = {english}
}

@inproceedings{petersen_systematic_2008,
	title = {Systematic Mapping Studies in Software Engineering},
	url = {https://scienceopen.com/document?vid=6d552894-2cc3-4e2b-a483-41fa48a37ef8},
	doi = {10.14236/ewic/EASE2008.8},
	abstract = {{BACKGROUND}: A software engineering systematic map is a deﬁned method to build a classiﬁcation scheme and structure a software engineering ﬁeld of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research ﬁeld can be determined. Different facets of the scheme can also be combined to answer more speciﬁc research questions.
{OBJECTIVE}: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.
{METHOD}: We have deﬁned a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.
{RESULTS}: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for doing systematic maps are deﬁned.
{CONCLUSIONS}: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
	eventtitle = {12th International Conference on Evaluation and Assessment in Software Engineering ({EASE})},
	author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
	urldate = {2019-10-16},
	date = {2008-06-01},
	langid = {english}
}

@article{liu_maximum_nodate,
	title = {Maximum Likelihood Evidential Reasoning-Based Hierarchical Inference with Incomplete Data},
	abstract = {Data mining requires a pre-processing task where data are prepared, cleaned, integrated, transformed, reduced and discretized to ensure data quality. Incomplete data are commonly encountered during data cleaning, which can have major impact on the conclusions that will be drawn from the data. In order to effectively carry out inferential modelling or decision making from incomplete independent variables or explanatory variables and consider different types of uncertainties, this paper adopts a data-driven inferential modelling approach, Maximum Likelihood Evidential Reasoning ({MAKER}) framework, which takes advantage of incomplete datasets without any imputation that may be required by other conventional machine learning methods. The {MAKER} framework reﬂects the plausibility of different values of missing data and expresses data-driven support for different values of missing data.},
	pages = {6},
	author = {Liu, Xi and Sachan, Swati and Yang, Jian-Bo and Xu, Dong-Ling},
	langid = {english},
	keywords = {maker}
}

@inproceedings{yang_inferential_2017,
	location = {Huddersfield, United Kingdom},
	title = {Inferential modelling and decision making with data},
	isbn = {978-0-7017-0260-1},
	url = {http://ieeexplore.ieee.org/document/8082048/},
	doi = {10.23919/IConAC.2017.8082048},
	abstract = {In this paper, we introduce the main concepts of a new maximum likelihood evidential reasoning ({MAKER}) framework for data-driven inferential modelling and decision making under different types of uncertainty. It consists of two types of model: state space model ({SSM}) and evidence space model ({ESM}), driven by the data that reflects the relationships between system inputs and output. {SSM} is constructed to describe different system states and changes. {ESM} is established by mapping data to a set of evidence that is partitioned into evidential elements each pointing to a system state set and together represents system behaviours in a probabilistic and distributed manner. The reliability of evidence and interdependence between a pair of evidence are explicitly measured. It is in the joint evidence-state space that multiple pieces of evidence with different degrees of interdependence and reliability are acquired from system inputs and combined to inference system output. A general optimal learning model is constructed, where evidence reliability can be learnt from historical data by maximising the likelihood of true state. In the {MAKER} framework, different types of uncertainty can be taken into account for inferential modelling, probabilistic prediction and decision making.},
	eventtitle = {2017 23rd International Conference on Automation and Computing ({ICAC})},
	pages = {1--6},
	booktitle = {2017 23rd International Conference on Automation and Computing ({ICAC})},
	publisher = {{IEEE}},
	author = {Yang, Jian-Bo and Xu, Dong-Ling},
	urldate = {2019-10-16},
	date = {2017-09},
	langid = {english},
	keywords = {data, todo}
}

@article{luque_equivalent_2007,
	title = {Equivalent Information for Multiobjective Interactive Procedures},
	volume = {53},
	issn = {0025-1909, 1526-5501},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0595},
	doi = {10.1287/mnsc.1060.0595},
	pages = {125--134},
	number = {1},
	journaltitle = {Management Science},
	shortjournal = {Management Science},
	author = {Luque, Mariano and Caballero, Rafael and Molina, Julian and Ruiz, Francisco},
	urldate = {2019-10-16},
	date = {2007-01},
	langid = {english},
	keywords = {info, preference, preferential, todo}
}

@article{chen_inference_2011,
	title = {Inference analysis and adaptive training for belief rule based systems},
	volume = {38},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741741100604X},
	doi = {10.1016/j.eswa.2011.04.077},
	abstract = {Belief rule base ({BRB}) systems are an extension of traditional {IF}-{THEN} rule based systems and capable of capturing complicated nonlinear causal relationships between antecedent attributes and consequents. In a {BRB} system, various types of information with uncertainties can be represented using belief structures, and a belief rule is designed with belief degrees embedded in its possible consequents. For a set of inputs to antecedent attributes, inference in {BRB} is implemented using the evidential reasoning ({ER}) approach. In this paper, the inference mechanism of the {ER} algorithm is analyzed ﬁrst and its patterns of monotonic inference and nonlinear approximation are revealed. For a practical {BRB} system, it is difﬁcult to determine its parameters accurately by using only experts’ subjective knowledge. Moreover, the appropriate adjustment of the parameters of a {BRB} system using available historical data can lead to signiﬁcant improvement on its prediction performance. In this paper, a training data selection scheme and an adaptive training method are developed for updating {BRB} parameters. Finally, numerical studies on a multimodal function and a practical pipeline leak detection problem are conducted to illustrate the functionality of {BRB} systems and validate the performance of the adaptive training technique.},
	pages = {12845--12860},
	number = {10},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Chen, Yu-Wang and Yang, Jian-Bo and Xu, Dong-Ling and Zhou, Zhi-Jie and Tang, Da-Wei},
	urldate = {2019-10-07},
	date = {2011-09},
	langid = {english}
}

@article{blasco_new_2008,
	title = {A new graphical visualization of n-dimensional Pareto front for decision-making in multiobjective optimization},
	volume = {178},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025508002016},
	doi = {10.1016/j.ins.2008.06.010},
	abstract = {New challenges in engineering design lead to multiobjective (multicriteria) problems. In this context, the Pareto front supplies a set of solutions where the designer (decision-maker) has to look for the best choice according to his preferences. Visualization techniques often play a key role in helping decision-makers, but they have important restrictions for more than two-dimensional Pareto fronts. In this work, a new graphical representation, called Level Diagrams, for n-dimensional Pareto front analysis is proposed. Level Diagrams consists of representing each objective and design parameter on separate diagrams. This new technique is based on two key points: classiﬁcation of Pareto front points according to their proximity to ideal points measured with a speciﬁc norm of normalized objectives (several norms can be used); and synchronization of objective and parameter diagrams. Some of the new possibilities for analyzing Pareto fronts are shown. Additionally, in order to introduce designer preferences, Level Diagrams can be coloured, so establishing a visual representation of preferences that can help the decision-maker. Finally, an example of a robust control design is presented – a benchmark proposed at the American Control Conference. This design is set as a six-dimensional multiobjective problem.},
	pages = {3908--3924},
	number = {20},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Blasco, X. and Herrero, J.M. and Sanchis, J. and Martínez, M.},
	urldate = {2019-10-02},
	date = {2008-10},
	langid = {english},
	keywords = {visualization}
}

@article{malle_integrating_2016,
	title = {Integrating robot ethics and machine morality: the study and design of moral competence in robots},
	volume = {18},
	issn = {1388-1957, 1572-8439},
	url = {http://link.springer.com/10.1007/s10676-015-9367-8},
	doi = {10.1007/s10676-015-9367-8},
	shorttitle = {Integrating robot ethics and machine morality},
	abstract = {Robot ethics encompasses ethical questions about how humans should design, deploy, and treat robots; machine morality encompasses questions about what moral capacities a robot should have and how these capacities could be computationally implemented. Publications on both of these topics have doubled twice in the past 10 years but have often remained separate from one another. In an attempt to better integrate the two, I offer a framework for what a morally competent robot would look like (normally considered machine morality) and discuss a number of ethical questions about the design, use, and treatment of such moral robots in society (normally considered robot ethics). Instead of searching for a ﬁxed set of criteria of a robot’s moral competence I identify the multiple elements that make up human moral competence and probe the possibility of designing robots that have one or more of these human elements, which include: moral vocabulary; a system of norms; moral cognition and affect; moral decision making and action; moral communication. Juxtaposing empirical research, philosophical debates, and computational challenges, this article adopts an optimistic perspective: if robotic design truly commits to building morally competent robots, then those robots could be trustworthy and productive partners, caretakers, educators, and members of the human community. Moral competence does not resolve all ethical concerns over robots in society, but it may be a prerequisite to resolve at least some of them.},
	pages = {243--256},
	number = {4},
	journaltitle = {Ethics and Information Technology},
	shortjournal = {Ethics Inf Technol},
	author = {Malle, Bertram F.},
	urldate = {2019-10-01},
	date = {2016-12},
	langid = {english}
}

@article{spence_information_2011,
	title = {Information, knowledge and wisdom: groundwork for the normative evaluation of digital information and its relation to the good life},
	volume = {13},
	issn = {1388-1957, 1572-8439},
	url = {http://link.springer.com/10.1007/s10676-011-9265-7},
	doi = {10.1007/s10676-011-9265-7},
	shorttitle = {Information, knowledge and wisdom},
	pages = {261--275},
	number = {3},
	journaltitle = {Ethics and Information Technology},
	shortjournal = {Ethics Inf Technol},
	author = {Spence, Edward H.},
	urldate = {2019-10-01},
	date = {2011-09},
	langid = {english}
}

@article{malmberg_data-driven_nodate,
	title = {Data-Driven Interactive Multiobjective Optimization using Cluster Based Surrogate in Discrete Decision Space},
	abstract = {This thesis presents a cluster based surrogate model approach for reducing dimension of discrete decision space and so for simplifying integer linear optimization problems. The model is especially aimed for solving data-driven decision making problems interactively, as the surrogate makes interaction more seamless and the interactive {NIMBUS} method manages well within the product space of the surrogate. The developed cluster based surrogate and method were also applied for a Boreal Forest management problem with promising results.},
	pages = {99},
	author = {Malmberg, Jose},
	langid = {english},
	keywords = {forest}
}

@article{monkkonen_spatially_2014,
	title = {Spatially dynamic forest management to sustain biodiversity and economic returns},
	volume = {134},
	issn = {03014797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030147971300769X},
	doi = {10.1016/j.jenvman.2013.12.021},
	abstract = {Production of marketed commodities and protection of biodiversity in natural systems often conﬂict and thus the continuously expanding human needs for more goods and beneﬁts from global ecosystems urgently calls for strategies to resolve this conﬂict. In this paper, we addressed what is the potential of a forest landscape to simultaneously produce habitats for species and economic returns, and how the conﬂict between habitat availability and timber production varies among taxa. Secondly, we aimed at revealing an optimal combination of management regimes that maximizes habitat availability for given levels of economic returns. We used multi-objective optimization tools to analyze data from a boreal forest landscape consisting of about 30,000 forest stands simulated 50 years into future. We included seven alternative management regimes, spanning from the recommended intensive forest management regime to complete set-aside of stands (protection), and ten different taxa representing a wide variety of habitat associations and social values. Our results demonstrate it is possible to achieve large improvements in habitat availability with little loss in economic returns. In general, providing dead-wood associated species with more habitats tended to be more expensive than providing requirements for other species. No management regime alone maximized habitat availability for the species, and systematic use of any single management regime resulted in considerable reductions in economic returns. Compared with an optimal combination of management regimes, a consistent application of the recommended management regime would result in 5\% reduction in economic returns and up to 270\% reduction in habitat availability. Thus, for all taxa a combination of management regimes was required to achieve the optimum. Refraining from silvicultural thinnings on a proportion of stands should be considered as a cost-effective management in commercial forests to reconcile the conﬂict between economic returns and habitat required by species associated with dead-wood. In general, a viable strategy to maintain biodiversity in production landscapes would be to diversify management regimes. Our results emphasize the importance of careful landscape level forest management planning because optimal combinations of management regimes were taxon-speciﬁc. For cost-efﬁciency, the results call for balanced and correctly targeted strategies among habitat types.},
	pages = {80--89},
	journaltitle = {Journal of Environmental Management},
	shortjournal = {Journal of Environmental Management},
	author = {Mönkkönen, Mikko and Juutinen, Artti and Mazziotta, Adriano and Miettinen, Kaisa and Podkopaev, Dmitry and Reunanen, Pasi and Salminen, Hannu and Tikkanen, Olli-Pekka},
	urldate = {2019-09-27},
	date = {2014-02},
	langid = {english},
	keywords = {forest}
}

@article{rasinmaki_simo:_2009,
	title = {{SIMO}: An adaptable simulation framework for multiscale forest resource data},
	volume = {66},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169908002469},
	doi = {10.1016/j.compag.2008.12.007},
	shorttitle = {{SIMO}},
	abstract = {Forest management planning is facing new objectives and diversiﬁed data sources. To succeed in the new context, a forest management planning framework should support integration of the new goals, knowledge, and technology as well as embrace multiple scales. The aim of the {SIMO} framework is to develop a hierarchical, extendable simulation and optimization framework for forest management planning. Current implementation includes a forest growth and yield simulator which is adaptable with respect to the components of a simulation; data, growth and operation models that modify the data, and the control structures between the data and the models. The simulation framework components implemented in {XML} include the hierarchical structures for data and simulation descriptions, and an extendable model base. To achieve ﬂexibility and reusability, the components are connected through a common ontology, again deﬁned in {XML}. The article includes two use cases demonstrating the adaptability of the simulation framework, which is available as open source software.},
	pages = {76--84},
	number = {1},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Computers and Electronics in Agriculture},
	author = {Rasinmäki, Jussi and Mäkinen, Antti and Kalliovirta, Jouni},
	urldate = {2019-09-27},
	date = {2009-04},
	langid = {english},
	keywords = {forest}
}

@article{eyvindson_mitigating_2018,
	title = {Mitigating forest biodiversity and ecosystem service losses in the era of bio-based economy},
	volume = {92},
	issn = {13899341},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389934117303210},
	doi = {10.1016/j.forpol.2018.04.009},
	abstract = {Forests play a crucial role in the transition towards a bioeconomy by providing biomass to substitute for fossilbased materials and energy. However, a policy-policy conﬂict exists between the desire to increase the utilization of bio based renewable resources and the desire to protect and conserve biodiversity. Increasing forest harvest levels to meet the needs of the bioeconomy may conﬂict with biodiversity protection and ecosystem services provided by forests. Through an optimization framework, we examined trade-oﬀs between increasing the extraction of timber resources, and the impacts on biodiversity and non-wood ecosystem services, and investigated possibilities to reconcile trade-oﬀ with changes in forest management in 17 landscapes in boreal forests. A diverse range of alternative forest management regimes were used. The alternatives varied from set aside to continuous cover forestry and a range of management options to reﬂect potential applications of the current management recommendations. These included adjustments to the number of thinning, the timing of ﬁnal felling and the method of regeneration. Increasing forest harvest level to the maximum economically sustainable harvest had a negative eﬀect on the habitat suitability index, bilberry yield, deadwood diversity and carbon storage. It resulted in a loss in variation among landscapes in their conservation capacity and the ability to provide ecosystem services. Multi-objective optimization results showed that combining diﬀerent forest management regimes alleviated the negative eﬀects of increasing harvest levels to biodiversity and non-wood ecosystem services. The results indicate that careful landscape level forest management planning is crucial to minimize the ecological costs of increasing harvest levels.},
	pages = {119--127},
	journaltitle = {Forest Policy and Economics},
	shortjournal = {Forest Policy and Economics},
	author = {Eyvindson, Kyle and Repo, Anna and Mönkkönen, Mikko},
	urldate = {2019-09-27},
	date = {2018-07},
	langid = {english},
	keywords = {forest}
}

@article{hsieh_three_2005,
	title = {Three Approaches to Qualitative Content Analysis},
	volume = {15},
	issn = {1049-7323, 1552-7557},
	url = {http://journals.sagepub.com/doi/10.1177/1049732305276687},
	doi = {10.1177/1049732305276687},
	pages = {1277--1288},
	number = {9},
	journaltitle = {Qualitative Health Research},
	shortjournal = {Qual Health Res},
	author = {Hsieh, Hsiu-Fang and Shannon, Sarah E.},
	urldate = {2019-09-25},
	date = {2005-11},
	langid = {english}
}

@article{braun_using_2006,
	title = {Using thematic analysis in psychology},
	volume = {3},
	issn = {1478-0887, 1478-0895},
	url = {http://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
	doi = {10.1191/1478088706qp063oa},
	pages = {77--101},
	number = {2},
	journaltitle = {Qualitative Research in Psychology},
	shortjournal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	urldate = {2019-09-25},
	date = {2006-01},
	langid = {english}
}

@article{sachan_probabilistic_2019,
	title = {Probabilistic dynamic programming algorithm: a solution for optimal maintenance policy for power cables},
	volume = {8},
	issn = {2520-1352, 2520-1360},
	url = {http://link.springer.com/10.1007/s41872-019-00074-3},
	doi = {10.1007/s41872-019-00074-3},
	shorttitle = {Probabilistic dynamic programming algorithm},
	abstract = {This paper presents a probabilistic dynamic programming algorithm to obtain the optimal cost-effective maintenance policy for a power cable. The algorithm determines the states which a cable might visit in the future and solves the functional equations of probabilistic dynamic programming by backward induction process. The optimisation model considers the probabilistic nature of cables failures. This work specifies the data needs, and presents a procedure to utilize maintenance data, failure data, cost data, and condition monitoring or diagnostic test data. The model can be used by power utility managers and regulators to assess the financial risk and schedule maintenance.},
	pages = {117--127},
	number = {2},
	journaltitle = {Life Cycle Reliability and Safety Engineering},
	shortjournal = {Life Cycle Reliab Saf Eng},
	author = {Sachan, Swati and Zhou, Chengke},
	urldate = {2019-09-12},
	date = {2019-06},
	langid = {english}
}

@article{noauthor_design_nodate,
	title = {Design Science in Information Systems Research},
	pages = {32},
	langid = {english}
}

@article{noauthor_design_nodate-1,
	title = {Design Science in Information Systems Research},
	pages = {32},
	langid = {english}
}

@incollection{noauthor_doing_2015,
	title = {Doing Bayesian Data Analysis, Second Edition},
	isbn = {978-0-12-405888-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124058880099992},
	pages = {i--ii},
	booktitle = {Doing Bayesian Data Analysis},
	publisher = {Elsevier},
	urldate = {2019-09-11},
	date = {2015},
	langid = {english},
	doi = {10.1016/B978-0-12-405888-0.09999-2}
}

@incollection{noauthor_front_2015,
	title = {Front Matter},
	isbn = {978-0-12-405888-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124058880099992},
	pages = {i--ii},
	booktitle = {Doing Bayesian Data Analysis},
	publisher = {Elsevier},
	urldate = {2019-09-11},
	date = {2015},
	langid = {english},
	doi = {10.1016/B978-0-12-405888-0.09999-2}
}

@article{xu_belief_2017,
	title = {A Belief Rule-Based Expert System for Fault Diagnosis of Marine Diesel Engines},
	issn = {2168-2216, 2168-2232},
	url = {http://ieeexplore.ieee.org/document/8082138/},
	doi = {10.1109/TSMC.2017.2759026},
	abstract = {This paper proposes a new belief rule-based ({BRB}) expert system for fault diagnosis of marine diesel engines. The expert system is the ﬁrst of its kind that consists of multiple concurrently activated {BRB} subsystems, in which each subsystem has its distinctive outputs and uses the evidential reasoning approach for inference. This novel modeling approach can be applied to identify fault modes that may co-exist. In essence, the group of {BRB} subsystems is used to model the nonlinear relationships between the fault features and the fault modes in marine diesel engines. The initial {BRB} expert system can be established by using expert experience and then optimized by using the data samples accumulated during the operation of marine diesel engines. Due to limitations in knowledge and data collected, ignorance is also considered in some {BRB} subsystems. The proposed {BRB} expert system is applied to abnormal wear detection for a kind of marine diesel engine. The performance of the {BRB} expert system is investigated in comparison with that of artiﬁcial neural network ({ANN}) models, support vector machine ({SVM}) models, and binary logistic regression model with ﬁvefold crossvalidation. The results show that the {BRB} expert system can be used for fault diagnosis of marine diesel engines in a probabilistic manner, which outperforms the {ANN} models, {SVM} models, and the binary logistic regression model in terms of accuracy and stability, and can effectively identify concurrent faults.},
	pages = {1--17},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
	shortjournal = {{IEEE} Trans. Syst. Man Cybern, Syst.},
	author = {Xu, Xiaojian and Yan, Xinping and Sheng, Chenxing and Yuan, Chengqing and Xu, Dongling and Yang, Jianbo},
	urldate = {2019-09-11},
	date = {2017},
	langid = {english}
}

@article{branke_learning_2015,
	title = {Learning Value Functions in Interactive Evolutionary Multiobjective Optimization},
	volume = {19},
	issn = {1089-778X, 1089-778X, 1941-0026},
	url = {http://ieeexplore.ieee.org/document/6729055/},
	doi = {10.1109/TEVC.2014.2303783},
	abstract = {This paper proposes an interactive multiobjective evolutionary algorithm ({MOEA}) that attempts to learn a value function capturing the users’ true preferences. At regular intervals, the user is asked to rank a single pair of solutions. This information is used to update the algorithm’s internal value function model, and the model is used in subsequent generations to rank solutions incomparable according to dominance. This speeds up evolution toward the region of the Pareto front that is most desirable to the user. We take into account the most general additive value function as a preference model and we empirically compare different ways to identify the value function that seems to be the most representative with respect to the given preference information, different types of user preferences, and different ways to use the learned value function in the {MOEA}. Results on a number of different scenarios suggest that the proposed algorithm works well over a range of benchmark problems and types of user preferences.},
	pages = {88--102},
	number = {1},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	shortjournal = {{IEEE} Trans. Evol. Computat.},
	author = {Branke, Jurgen and Greco, Salvatore and Slowinski, Roman and Zielniewicz, Piotr},
	urldate = {2019-09-10},
	date = {2015-02},
	langid = {english}
}

@article{battiti_braincomputer_2010,
	title = {Brain–Computer Evolutionary Multiobjective Optimization: A Genetic Algorithm Adapting to the Decision Maker},
	volume = {14},
	issn = {1089-778X, 1941-0026},
	url = {http://ieeexplore.ieee.org/document/5560789/},
	doi = {10.1109/TEVC.2010.2058118},
	shorttitle = {Brain–Computer Evolutionary Multiobjective Optimization},
	abstract = {The centrality of the decision maker ({DM}) is widely recognized in the multiple criteria decision-making community. This translates into emphasis on seamless human–computer interaction, and adaptation of the solution technique to the knowledge which is progressively acquired from the {DM}. This paper adopts the methodology of reactive search optimization ({RSO}) for evolutionary interactive multiobjective optimization. {RSO} follows to the paradigm of “learning while optimizing,” through the use of online machine learning techniques as an integral part of a self-tuning optimization scheme. User judgments of couples of solutions are used to build robust incremental models of the user utility function, with the objective to reduce the cognitive burden required from the {DM} to identify a satisﬁcing solution. The technique of support vector ranking is used together with a k-fold cross-validation procedure to select the best kernel for the problem at hand, during the utility function training procedure. Experimental results are presented for a series of benchmark problems.},
	pages = {671--687},
	number = {5},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	shortjournal = {{IEEE} Trans. Evol. Computat.},
	author = {Battiti, Roberto and Passerini, Andrea},
	urldate = {2019-09-09},
	date = {2010-10},
	langid = {english}
}

@article{shen_multi-objective_2010,
	title = {A multi-objective optimization evolutionary algorithm incorporating preference information based on fuzzy logic},
	volume = {46},
	issn = {0926-6003, 1573-2894},
	url = {http://link.springer.com/10.1007/s10589-008-9189-2},
	doi = {10.1007/s10589-008-9189-2},
	abstract = {A multi-objective optimization evolutionary algorithm incorporating preference information interactively is proposed. A new nine grade evaluation method is used to quantify the linguistic preferences expressed by the decision maker ({DM}) so as to reduce his/her cognitive overload. When comparing individuals, the classical Pareto dominance relation is commonly used, but it has difﬁculty in dealing with problems involving large numbers of objectives in which it gives an unmanageable and large set of Pareto optimal solutions. In order to overcome this limitation, a new outranking relation called “strength superior” which is based on the preference information is constructed via a fuzzy inference system to help the algorithm ﬁnd a few solutions located in the preferred regions, and the graphical user interface is used to realize the interaction between the {DM} and the algorithm. The computational complexity of the proposed algorithm is analyzed theoretically, and its ability to handle preference information is validated through simulation. The inﬂuence of parameters on the performance of the algorithm is discussed and comparisons to another preference guided multi-objective evolutionary algorithm indicate that the proposed algorithm is effective in solving high dimensional optimization problems.},
	pages = {159--188},
	number = {1},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Shen, Xiaoning and Guo, Yu and Chen, Qingwei and Hu, Weili},
	urldate = {2019-09-07},
	date = {2010-05},
	langid = {english},
	keywords = {read}
}

@article{xin_interactive_2018,
	title = {Interactive Multiobjective Optimization: A Review of the State-of-the-Art},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8412189/},
	doi = {10.1109/ACCESS.2018.2856832},
	shorttitle = {Interactive Multiobjective Optimization},
	abstract = {Interactive multiobjective optimization ({IMO}) aims at ﬁnding the most preferred solution of a decision maker with the guidance of his/her preferences which are provided progressively. During the process, the decision maker can adjust his/her preferences and explore only interested regions of the search space. In recent decades, {IMO} has gradually become a common interest of two distinct communities, namely, the multiple criteria decision making ({MCDM}) and the evolutionary multiobjective optimization ({EMO}). The {IMO} methods developed by the {MCDM} community usually use the mathematical programming methodology to search for a single preferred Pareto optimal solution, while those which are rooted in {EMO} often employ evolutionary algorithms to generate a representative set of solutions in the decision maker’s preferred region. This paper aims to give a review of {IMO} research from both {MCDM} and {EMO} perspectives. Taking into account four classiﬁcation criteria including the interaction pattern, preference information, preference model, and search engine (i.e., optimization algorithm), a taxonomy is established to identify important {IMO} factors and differentiate various {IMO} methods. According to the taxonomy, state-of-the-art {IMO} methods are categorized and reviewed and the design ideas behind them are summarized. A collection of important issues, e.g., the burdens, cognitive biases and preference inconsistency of decision makers, and the performance measures and metrics for evaluating {IMO} methods, are highlighted and discussed. Several promising directions worthy of future research are also presented.},
	pages = {41256--41279},
	journaltitle = {{IEEE} Access},
	author = {Xin, Bin and Chen, Lu and Chen, Jie and Ishibuchi, Hisao and Hirota, Kaoru and Liu, Bo},
	urldate = {2019-09-06},
	date = {2018},
	langid = {english},
	keywords = {read, review}
}

@article{kong_belief_2016,
	title = {Belief rule-based inference for predicting trauma outcome},
	volume = {95},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705115004645},
	doi = {10.1016/j.knosys.2015.12.002},
	abstract = {A belief rule-based inference methodology using the evidential reasoning approach ({RIMER}) is employed in this study to construct a decision support tool that helps physicians predict in-hospital death and intensive care unit admission among trauma patients in emergency departments ({EDs}). This study contributes to the research community by developing and validating a {RIMER}-based decision tool for predicting trauma outcome. To compare the prediction performance of the {RIMER} model with those of models derived using commonly adopted methods, such as logistic regression analysis, support vector machine ({SVM}), and artiﬁcial neural network ({ANN}), several logistic regression models, {SVM} models, and {ANN} models are constructed using the same dataset. Five-fold cross-validation is employed to train and validate the prediction models constructed using four different methods. Results indicate that the {RIMER} model has the best prediction performance among the four models, and its performance can be improved after knowledge base training with historical data. The {RIMER} tool exhibits strong potential to help {ED} physicians to better triage trauma, optimally utilize hospital resources, and achieve better patient outcomes.},
	pages = {35--44},
	journaltitle = {Knowledge-Based Systems},
	author = {Kong, Guilan and Xu, Dong-Ling and Yang, Jian-Bo and Yin, Xiaofeng and Wang, Tianbing and Jiang, Baoguo and Hu, Yonghua},
	urldate = {2019-09-06},
	date = {2016-03},
	langid = {english},
	keywords = {read}
}

@article{jaques_rule-based_2013,
	title = {Rule-based expert systems to support step-by-step guidance in algebraic problem solving: The case of the tutor {PAT}2Math},
	volume = {40},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417413002418},
	doi = {10.1016/j.eswa.2013.04.004},
	shorttitle = {Rule-based expert systems to support step-by-step guidance in algebraic problem solving},
	abstract = {In order for an Intelligent Tutoring System ({ITS}) to correct students’ exercises, it must know how to solve the same type of problems that students do and the related knowledge components. It can, thereby, compare the desirable solution with the student’s answer. This task can be accomplished by an expert system. However, it has some drawbacks, such as an exponential complexity time, which impairs the desirable real-time response. In this paper we describe the expert system ({ES}) module of an Algebra {ITS}, called {PAT}2Math. The {ES} is responsible for correcting student steps and modeling student knowledge components during equations problem solving. Another important function of this module is to demonstrate to students how to solve a problem. In this paper, we focus mainly on the implementation of this module as a rule-based expert system. We also describe how we reduced the complexity of this module from O(nd) to O(d), where n is the number of rules in the knowledge base, by implementing some meta-rules that aim at inferring the operations students applied in order to produce a step. We evaluated our approach through a user study with forty-three seventh grade students. The students who interacted with our tool showed statistically higher scores on equation solving tests, after solving algebra exercises with {PAT}2Math during an approximately two-hour session, than students who solved the same exercises using only paper and pencil.},
	pages = {5456--5465},
	number = {14},
	journaltitle = {Expert Systems with Applications},
	author = {Jaques, Patricia A. and Seffrin, Henrique and Rubi, Geiseane and de Morais, Felipe and Ghilardi, Cássio and Bittencourt, Ig Ibert and Isotani, Seiji},
	urldate = {2019-09-06},
	date = {2013-10},
	langid = {english},
	keywords = {read}
}

@article{zhou_sequential_2010,
	title = {A sequential learning algorithm for online constructing belief-rule-based systems},
	volume = {37},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417409007246},
	doi = {10.1016/j.eswa.2009.07.067},
	abstract = {A belief rule base inference methodology using the evidential reasoning ({RIMER}) approach has been developed recently. A belief rule base ({BRB}), which can be treated as a more generalized expert system, extends traditional {IF}-{THEN} rules, but requires the assignment of some system parameters including rule weights, attribute weights, and belief degrees. These parameters need to be determined with care for reliable system simulation and prediction. Some off-line optimization models have been proposed, but it is expensive to train and re-train these models in particular for large-scale systems. Moreover, the recursive algorithms are also proposed to ﬁne tune a {BRB} online, which require less calculation time and satisfy the real-time requirement. However, the earlier mentioned learning algorithms are all based on a predetermined structure of the {BRB}. For a complex system, prior knowledge may not be perfect, which leads to the construction of an incomplete or even inappropriate initial {BRB} structure. Also, too many rules in an initial {BRB} may lead to over ﬁtting, whilst too few rules may result in under ﬁtting. Consequently, such a {BRB} system may not be capable of achieving overall optimal performance. In this paper, we consider one realistic and important case where both a preliminary {BRB} structure and system parameters assigned to given rules can be adjusted online. Based on the deﬁnition of a new statistical utility for a belief rule as investigated in this paper, a sequential learning algorithm for online constructing more compact {BRB} systems is proposed. Compared with the other learning algorithms, a belief rule can be automatically added into the {BRB} or pruned from the {BRB}, and our algorithm can also satisfy the real-time requirement. In addition, our algorithm inherits the feature of {RIMER}, i.e., only partial input and output information is required, which could be either incomplete or vague, either numerical or judgmental, or mixed. In order to verify the effectiveness of the proposed algorithm, a practical case study about oil pipeline leak detection is studied and examined to demonstrate how the algorithm can be implemented.},
	pages = {1790--1799},
	number = {2},
	journaltitle = {Expert Systems with Applications},
	author = {Zhou, Zhi-Jie and Hu, Chang-Hua and Yang, Jian-Bo and Xu, Dong-Ling and Chen, Mao-Yin and Zhou, Dong-Hua},
	urldate = {2019-09-06},
	date = {2010-03},
	langid = {english},
	keywords = {read}
}

@collection{branke_multiobjective_2008,
	location = {Berlin, Heidelberg},
	title = {Multiobjective Optimization: Interactive and Evolutionary Approaches},
	volume = {5252},
	isbn = {978-3-540-88907-6 978-3-540-88908-3},
	url = {http://link.springer.com/10.1007/978-3-540-88908-3},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Multiobjective Optimization},
	publisher = {Springer Berlin Heidelberg},
	editor = {Branke, Jürgen and Deb, Kalyanmoy and Miettinen, Kaisa and Słowiński, Roman},
	urldate = {2019-09-04},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-3-540-88908-3}
}

@article{goos_lecture_nodate,
	title = {Lecture Notes in Computer Science},
	pages = {481},
	author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
	langid = {english}
}

@article{yang_optimization_2007,
	title = {Optimization Models for Training Belief-Rule-Based Systems},
	volume = {37},
	issn = {1083-4427},
	url = {http://ieeexplore.ieee.org/document/4244562/},
	doi = {10.1109/TSMCA.2007.897606},
	abstract = {A belief Rule-base Inference Methodology using the Evidential Reasoning approach ({RIMER}) has been developed recently, where a new belief rule representation scheme is proposed to extend traditional {IF}–{THEN} rules. The belief rule expression matrix in {RIMER} provides a compact framework for representing expert knowledge. However, it is difﬁcult to accurately determine the parameters of a belief rule base ({BRB}) entirely subjectively, particularly, for a large-scale {BRB} with hundreds or even thousands of rules. In addition, a change in rule weight or attribute weight may lead to changes in the performance of a {BRB}. As such, there is a need to develop a supporting mechanism that can be used to train, in a locally optimal way, a {BRB} that is initially built using expert knowledge. In this paper, several new optimization models for locally training a {BRB} are developed. The new models are either single- or multiple-objective nonlinear optimization problems. The main feature of these new models is that only partial input and output information is required, which can be either incomplete or vague, either numerical or judgmental, or mixed. The models can be used to ﬁne tune a {BRB} whose internal structure is initially decided by experts’ domain-speciﬁc knowledge or common sense judgments. As such, a wide range of knowledge representation schemes can be handled, thereby facilitating the construction of various types of {BRB} systems. Conclusions drawn from such a trained {BRB} with partially built-in expert knowledge can simulate real situations in a meaningful, consistent, and locally optimal way. A numerical study for a hierarchical rule base is examined to demonstrate how the new models can be implemented as well as their potential applications.},
	pages = {569--585},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Yang, Jian-Bo and Liu, Jun and Xu, Dong-Ling and Wang, Jin and Wang, Hongwei},
	urldate = {2019-09-04},
	date = {2007-07},
	langid = {english}
}

@article{yang_evidential_2006,
	title = {The evidential reasoning approach for {MADA} under both probabilistic and fuzzy uncertainties},
	volume = {171},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704006277},
	doi = {10.1016/j.ejor.2004.09.017},
	abstract = {Many multiple attribute decision analysis ({MADA}) problems are characterised by both quantitative and qualitative attributes with various types of uncertainties. Incompleteness (or ignorance) and vagueness (or fuzziness) are among the most common uncertainties in decision analysis. The evidential reasoning ({ER}) approach has been developed in the 1990s and in the recent years to support the solution of {MADA} problems with ignorance, a kind of probabilistic uncertainty. In this paper, the {ER} approach is further developed to deal with {MADA} problems with both probabilistic and fuzzy uncertainties.},
	pages = {309--343},
	number = {1},
	journaltitle = {European Journal of Operational Research},
	author = {Yang, J.B. and Wang, Y.M. and Xu, D.L. and Chin, K.S.},
	urldate = {2019-09-04},
	date = {2006-05},
	langid = {english}
}

@article{xu_inference_2007,
	title = {Inference and learning methodology of belief-rule-based expert system for pipeline leak detection},
	volume = {32},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741740500312X},
	doi = {10.1016/j.eswa.2005.11.015},
	abstract = {Belief rule based expert systems are an extension of traditional rule based systems and are capable of representing more complicated causal relationships using diﬀerent types of information with uncertainties. This paper describes how the belief rule based expert systems can be trained and used for pipeline leak detection. Pipeline operations under diﬀerent conditions are modelled by a belief rule base using expert knowledge, which is then trained and ﬁne tuned using pipeline operating data, and validated by testing data. All training and testing data are collected and scaled from a real pipeline. The study demonstrates that the belief rule based system is ﬂexible, can be adapted to represent complicated expert systems, and is a valid novel approach for pipeline leak detection.},
	pages = {103--113},
	number = {1},
	journaltitle = {Expert Systems with Applications},
	author = {Xu, Dong-Ling and Liu, Jun and Yang, Jian-Bo and Liu, Guo-Ping and Wang, Jin and Jenkinson, Ian and Ren, Jun},
	urldate = {2019-09-04},
	date = {2007-01},
	langid = {english}
}

@article{miettinen_nautilus_2010,
	title = {{NAUTILUS} method: An interactive technique in multiobjective optimization based on the nadir point},
	volume = {206},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221710001785},
	doi = {10.1016/j.ejor.2010.02.041},
	shorttitle = {{NAUTILUS} method},
	abstract = {Most interactive methods developed for solving multiobjective optimization problems sequentially generate Pareto optimal or nondominated vectors and the decision maker must always allow impairment in at least one objective function to get a new solution. The {NAUTILUS} method proposed is based on the assumptions that past experiences affect decision makers’ hopes and that people do not react symmetrically to gains and losses. Therefore, some decision makers may prefer to start from the worst possible objective values and to improve every objective step by step according to their preferences. In {NAUTILUS}, starting from the nadir point, a solution is obtained at each iteration which dominates the previous one. Although only the last solution will be Pareto optimal, the decision maker never looses sight of the Pareto optimal set, and the search is oriented so that (s)he progressively focusses on the preferred part of the Pareto optimal set. Each new solution is obtained by minimizing an achievement scalarizing function including preferences about desired improvements in objective function values. {NAUTILUS} is specially suitable for avoiding undesired anchoring effects, for example in negotiation support problems, or just as a means of ﬁnding an initial Pareto optimal solution for any interactive procedure. An illustrative example demonstrates how this new method iterates.},
	pages = {426--434},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	author = {Miettinen, Kaisa and Eskelinen, Petri and Ruiz, Francisco and Luque, Mariano},
	urldate = {2019-09-04},
	date = {2010-10},
	langid = {english}
}

@article{jian-bo_yang_belief_2006,
	title = {Belief rule-base inference methodology using the evidential reasoning Approach-{RIMER}},
	volume = {36},
	issn = {1083-4427},
	url = {http://ieeexplore.ieee.org/document/1597400/},
	doi = {10.1109/TSMCA.2005.851270},
	abstract = {In this paper, a generic rule-base inference methodology using the evidential reasoning ({RIMER}) approach is proposed. Existing knowledge-base structures are ﬁrst examined, and knowledge representation schemes under uncertainty are then brieﬂy analyzed. Based on this analysis, a new knowledge representation scheme in a rule base is proposed using a belief structure. In this scheme, a rule base is designed with belief degrees embedded in all possible consequents of a rule. Such a rule base is capable of capturing vagueness, incompleteness, and nonlinear causal relationships, while traditional if–then rules can be represented as a special case. Other knowledge representation parameters such as the weights of both attributes and rules are also investigated in the scheme. In an established rule base, an input to an antecedent attribute is transformed into a belief distribution. Subsequently, inference in such a rule base is implemented using the evidential reasoning ({ER}) approach. The scheme is further extended to inference in hierarchical rule bases. A numerical study is provided to illustrate the potential applications of the proposed methodology.},
	pages = {266--285},
	number = {2},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {{Jian-Bo Yang} and {Jun Liu} and {Jin Wang} and {How-Sing Sii} and {Hong-Wei Wang}},
	urldate = {2019-09-04},
	date = {2006-03},
	langid = {english}
}

@article{goos_lecture_nodate-1,
	title = {Lecture Notes in Computer Science},
	pages = {481},
	author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
	langid = {english}
}

@inproceedings{dosilovic_explainable_2018,
	location = {Opatija},
	title = {Explainable artificial intelligence: A survey},
	isbn = {978-953-233-095-3},
	url = {https://ieeexplore.ieee.org/document/8400040/},
	doi = {10.23919/MIPRO.2018.8400040},
	shorttitle = {Explainable artificial intelligence},
	abstract = {In the last decade, with availability of large datasets and more com puting power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-ofthe-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. health care and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence ({XAI}) has become an area of interest in research community. This paper summarizes recent developments in {XAI} in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
	eventtitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	pages = {0210--0215},
	booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	publisher = {{IEEE}},
	author = {Dosilovic, Filip Karlo and Brcic, Mario and Hlupic, Nikica},
	urldate = {2019-09-04},
	date = {2018-05},
	langid = {english}
}

@article{ruiz_e-nautilus:_2015,
	title = {E-{NAUTILUS}: A decision support system for complex multiobjective optimization problems based on the {NAUTILUS} method},
	volume = {246},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221715003203},
	doi = {10.1016/j.ejor.2015.04.027},
	shorttitle = {E-{NAUTILUS}},
	abstract = {Interactive multiobjective optimization methods cannot necessarily be easily used when (industrial) multiobjective optimization problems are involved. There are at least two important factors to be considered with any interactive method: computationally expensive functions and aspects of human behavior. In this paper, we propose a method based on the existing {NAUTILUS} method and call it the Enhanced {NAUTILUS} (E-{NAUTILUS}) method. This method borrows the motivation of {NAUTILUS} along with the human aspects related to avoiding trading-off and anchoring bias and extends its applicability for computationally expensive multiobjective optimization problems. In the E-{NAUTILUS} method, a set of Pareto optimal solutions is calculated in a pre-processing stage before the decision maker is involved. When the decision maker interacts with the solution process in the interactive decision making stage, no new optimization problem is solved, thus, avoiding the waiting time for the decision maker to obtain new solutions according to her/his preferences. In this stage, starting from the worst possible objective function values, the decision maker is shown a set of points in the objective space, from which (s)he chooses one as the preferable point. At successive iterations, (s)he always sees points which improve all the objective values achieved by the previously chosen point. In this way, the decision maker remains focused on the solution process, as there is no loss in any objective function value between successive iterations. The last post-processing stage ensures the Pareto optimality of the ﬁnal solution. A real-life engineering problem is used to demonstrate how E-{NAUTILUS} works in practice. © 2015 Elsevier B.V. and Association of European Operational Research Societies ({EURO}) within the International Federation of Operational Research Societies ({IFORS}). All rights reserved.},
	pages = {218--231},
	number = {1},
	journaltitle = {European Journal of Operational Research},
	author = {Ruiz, Ana B. and Sindhya, Karthik and Miettinen, Kaisa and Ruiz, Francisco and Luque, Mariano},
	urldate = {2019-09-04},
	date = {2015-10},
	langid = {english}
}

@inproceedings{ribeiro_why_2016,
	location = {San Francisco, California, {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	eventtitle = {the 22nd {ACM} {SIGKDD} International Conference},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} '16},
	publisher = {{ACM} Press},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2019-09-04},
	date = {2016},
	langid = {english}
}

@article{miettinen_synchronous_2006,
	title = {Synchronous approach in interactive multiobjective optimization},
	volume = {170},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704005260},
	doi = {10.1016/j.ejor.2004.07.052},
	abstract = {We introduce a new approach in the methodology development for interactive multiobjective optimization. The presentation is given in the context of the interactive {NIMBUS} method, where the solution process is based on the classiﬁcation of objective functions. The idea is to formulate several scalarizing functions, all using the same preference information of the decision maker. Thus, opposed to ﬁxing one scalarizing function (as is done in most methods), we utilize several scalarizing functions in a synchronous way. This means that we as method developers do not make the choice between diﬀerent scalarizing functions but calculate the results of diﬀerent scalarizing functions and leave the ﬁnal decision to the expert, the decision maker. Simultaneously, (s)he obtains a better view of the solutions corresponding to her/his preferences expressed once during each iteration.},
	pages = {909--922},
	number = {3},
	journaltitle = {European Journal of Operational Research},
	author = {Miettinen, Kaisa and Mäkelä, Marko M.},
	urldate = {2019-09-04},
	date = {2006-05},
	langid = {english}
}

@article{goos_lecture_nodate-2,
	title = {Lecture Notes in Computer Science},
	pages = {481},
	author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
	langid = {english}
}

@article{jian-bo_yang_belief_2006-1,
	title = {Belief rule-base inference methodology using the evidential reasoning Approach-{RIMER}},
	volume = {36},
	issn = {1083-4427},
	url = {http://ieeexplore.ieee.org/document/1597400/},
	doi = {10.1109/TSMCA.2005.851270},
	abstract = {In this paper, a generic rule-base inference methodology using the evidential reasoning ({RIMER}) approach is proposed. Existing knowledge-base structures are ﬁrst examined, and knowledge representation schemes under uncertainty are then brieﬂy analyzed. Based on this analysis, a new knowledge representation scheme in a rule base is proposed using a belief structure. In this scheme, a rule base is designed with belief degrees embedded in all possible consequents of a rule. Such a rule base is capable of capturing vagueness, incompleteness, and nonlinear causal relationships, while traditional if–then rules can be represented as a special case. Other knowledge representation parameters such as the weights of both attributes and rules are also investigated in the scheme. In an established rule base, an input to an antecedent attribute is transformed into a belief distribution. Subsequently, inference in such a rule base is implemented using the evidential reasoning ({ER}) approach. The scheme is further extended to inference in hierarchical rule bases. A numerical study is provided to illustrate the potential applications of the proposed methodology.},
	pages = {266--285},
	number = {2},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {{Jian-Bo Yang} and {Jun Liu} and {Jin Wang} and {How-Sing Sii} and {Hong-Wei Wang}},
	urldate = {2019-09-02},
	date = {2006-03},
	langid = {english}
}

@article{yang_optimization_2007-1,
	title = {Optimization Models for Training Belief-Rule-Based Systems},
	volume = {37},
	issn = {1083-4427},
	url = {http://ieeexplore.ieee.org/document/4244562/},
	doi = {10.1109/TSMCA.2007.897606},
	abstract = {A belief Rule-base Inference Methodology using the Evidential Reasoning approach ({RIMER}) has been developed recently, where a new belief rule representation scheme is proposed to extend traditional {IF}–{THEN} rules. The belief rule expression matrix in {RIMER} provides a compact framework for representing expert knowledge. However, it is difﬁcult to accurately determine the parameters of a belief rule base ({BRB}) entirely subjectively, particularly, for a large-scale {BRB} with hundreds or even thousands of rules. In addition, a change in rule weight or attribute weight may lead to changes in the performance of a {BRB}. As such, there is a need to develop a supporting mechanism that can be used to train, in a locally optimal way, a {BRB} that is initially built using expert knowledge. In this paper, several new optimization models for locally training a {BRB} are developed. The new models are either single- or multiple-objective nonlinear optimization problems. The main feature of these new models is that only partial input and output information is required, which can be either incomplete or vague, either numerical or judgmental, or mixed. The models can be used to ﬁne tune a {BRB} whose internal structure is initially decided by experts’ domain-speciﬁc knowledge or common sense judgments. As such, a wide range of knowledge representation schemes can be handled, thereby facilitating the construction of various types of {BRB} systems. Conclusions drawn from such a trained {BRB} with partially built-in expert knowledge can simulate real situations in a meaningful, consistent, and locally optimal way. A numerical study for a hierarchical rule base is examined to demonstrate how the new models can be implemented as well as their potential applications.},
	pages = {569--585},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Yang, Jian-Bo and Liu, Jun and Xu, Dong-Ling and Wang, Jin and Wang, Hongwei},
	urldate = {2019-09-02},
	date = {2007-07},
	langid = {english}
}

@article{xu_inference_2007-1,
	title = {Inference and learning methodology of belief-rule-based expert system for pipeline leak detection},
	volume = {32},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741740500312X},
	doi = {10.1016/j.eswa.2005.11.015},
	abstract = {Belief rule based expert systems are an extension of traditional rule based systems and are capable of representing more complicated causal relationships using diﬀerent types of information with uncertainties. This paper describes how the belief rule based expert systems can be trained and used for pipeline leak detection. Pipeline operations under diﬀerent conditions are modelled by a belief rule base using expert knowledge, which is then trained and ﬁne tuned using pipeline operating data, and validated by testing data. All training and testing data are collected and scaled from a real pipeline. The study demonstrates that the belief rule based system is ﬂexible, can be adapted to represent complicated expert systems, and is a valid novel approach for pipeline leak detection.},
	pages = {103--113},
	number = {1},
	journaltitle = {Expert Systems with Applications},
	author = {Xu, Dong-Ling and Liu, Jun and Yang, Jian-Bo and Liu, Guo-Ping and Wang, Jin and Jenkinson, Ian and Ren, Jun},
	urldate = {2019-09-02},
	date = {2007-01},
	langid = {english}
}

@inproceedings{dosilovic_explainable_2018-1,
	location = {Opatija},
	title = {Explainable artificial intelligence: A survey},
	isbn = {978-953-233-095-3},
	url = {https://ieeexplore.ieee.org/document/8400040/},
	doi = {10.23919/MIPRO.2018.8400040},
	shorttitle = {Explainable artificial intelligence},
	abstract = {In the last decade, with availability of large datasets and more com puting power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-ofthe-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. health care and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence ({XAI}) has become an area of interest in research community. This paper summarizes recent developments in {XAI} in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
	eventtitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	pages = {0210--0215},
	booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({MIPRO})},
	publisher = {{IEEE}},
	author = {Dosilovic, Filip Karlo and Brcic, Mario and Hlupic, Nikica},
	urldate = {2019-09-02},
	date = {2018-05},
	langid = {english}
}

@inproceedings{ribeiro_why_2016-1,
	location = {San Francisco, California, {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	eventtitle = {the 22nd {ACM} {SIGKDD} International Conference},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} '16},
	publisher = {{ACM} Press},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2019-09-02},
	date = {2016},
	langid = {english}
}
@article{miettinen_nautilus_2010,
	title = {{NAUTILUS} method: An interactive technique in multiobjective optimization based on the nadir point},
	volume = {206},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221710001785},
	doi = {10.1016/j.ejor.2010.02.041},
	shorttitle = {{NAUTILUS} method},
	abstract = {Most interactive methods developed for solving multiobjective optimization problems sequentially generate Pareto optimal or nondominated vectors and the decision maker must always allow impairment in at least one objective function to get a new solution. The {NAUTILUS} method proposed is based on the assumptions that past experiences affect decision makers’ hopes and that people do not react symmetrically to gains and losses. Therefore, some decision makers may prefer to start from the worst possible objective values and to improve every objective step by step according to their preferences. In {NAUTILUS}, starting from the nadir point, a solution is obtained at each iteration which dominates the previous one. Although only the last solution will be Pareto optimal, the decision maker never looses sight of the Pareto optimal set, and the search is oriented so that (s)he progressively focusses on the preferred part of the Pareto optimal set. Each new solution is obtained by minimizing an achievement scalarizing function including preferences about desired improvements in objective function values. {NAUTILUS} is specially suitable for avoiding undesired anchoring effects, for example in negotiation support problems, or just as a means of ﬁnding an initial Pareto optimal solution for any interactive procedure. An illustrative example demonstrates how this new method iterates.},
	pages = {426--434},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	author = {Miettinen, Kaisa and Eskelinen, Petri and Ruiz, Francisco and Luque, Mariano},
	urldate = {2019-09-02},
	date = {2010-10},
	langid = {english}
}

@article{miettinen_synchronous_2006,
	title = {Synchronous approach in interactive multiobjective optimization},
	volume = {170},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704005260},
	doi = {10.1016/j.ejor.2004.07.052},
	abstract = {We introduce a new approach in the methodology development for interactive multiobjective optimization. The presentation is given in the context of the interactive {NIMBUS} method, where the solution process is based on the classiﬁcation of objective functions. The idea is to formulate several scalarizing functions, all using the same preference information of the decision maker. Thus, opposed to ﬁxing one scalarizing function (as is done in most methods), we utilize several scalarizing functions in a synchronous way. This means that we as method developers do not make the choice between diﬀerent scalarizing functions but calculate the results of diﬀerent scalarizing functions and leave the ﬁnal decision to the expert, the decision maker. Simultaneously, (s)he obtains a better view of the solutions corresponding to her/his preferences expressed once during each iteration.},
	pages = {909--922},
	number = {3},
	journaltitle = {European Journal of Operational Research},
	author = {Miettinen, Kaisa and Mäkelä, Marko M.},
	urldate = {2019-09-02},
	date = {2006-05},
	langid = {english}
}

@article{yang_evidential_2006,
	title = {The evidential reasoning approach for {MADA} under both probabilistic and fuzzy uncertainties},
	volume = {171},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704006277},
	doi = {10.1016/j.ejor.2004.09.017},
	abstract = {Many multiple attribute decision analysis ({MADA}) problems are characterised by both quantitative and qualitative attributes with various types of uncertainties. Incompleteness (or ignorance) and vagueness (or fuzziness) are among the most common uncertainties in decision analysis. The evidential reasoning ({ER}) approach has been developed in the 1990s and in the recent years to support the solution of {MADA} problems with ignorance, a kind of probabilistic uncertainty. In this paper, the {ER} approach is further developed to deal with {MADA} problems with both probabilistic and fuzzy uncertainties.},
	pages = {309--343},
	number = {1},
	journaltitle = {European Journal of Operational Research},
	author = {Yang, J.B. and Wang, Y.M. and Xu, D.L. and Chin, K.S.},
	urldate = {2019-09-02},
	date = {2006-05},
	langid = {english}
}

@article{qingfu_zhang_moea/d:_2007,
	title = {{MOEA}/D: A Multiobjective Evolutionary Algorithm Based on Decomposition},
	volume = {11},
	issn = {1941-0026, 1089-778X},
	url = {http://ieeexplore.ieee.org/document/4358754/},
	doi = {10.1109/TEVC.2007.892759},
	shorttitle = {{MOEA}/D},
	abstract = {Decomposition is a basic strategy in traditional multiobjective optimization. However, it has not yet been widely used in multiobjective evolutionary optimization. This paper proposes a multiobjective evolutionary algorithm based on decomposition ({MOEA}/D). It decomposes a multiobjective optimization problem into a number of scalar optimization subproblems and optimizes them simultaneously. Each subproblem is optimized by only using information from its several neighboring subproblems, which makes {MOEA}/D have lower computational complexity at each generation than {MOGLS} and nondominated sorting genetic algorithm {II} ({NSGA}-{II}). Experimental results have demonstrated that {MOEA}/D with simple decomposition methods outperforms or performs similarly to {MOGLS} and {NSGA}-{II} on multiobjective 0–1 knapsack problems and continuous multiobjective optimization problems. It has been shown that {MOEA}/D using objective normalization can deal with disparately-scaled objectives, and {MOEA}/D with an advanced decomposition method can generate a set of very evenly distributed solutions for 3-objective test instances. The ability of {MOEA}/D with small population, the scalability and sensitivity of {MOEA}/D have also been experimentally investigated in this paper.},
	pages = {712--731},
	number = {6},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	author = {{Qingfu Zhang} and {Hui Li}},
	urldate = {2019-09-02},
	date = {2007-12},
	langid = {english}
}

@book{chollet_deep_2018,
	location = {Shelter Island, New York},
	title = {Deep learning with Python},
	isbn = {978-1-61729-443-3},
	pagetotal = {361},
	publisher = {Manning Publications Co},
	author = {Chollet, François},
	date = {2018},
	langid = {english},
	note = {{OCLC}: ocn982650571},
	keywords = {Machine learning, Neural networks (Computer science), Python (Computer program language)}
}

@book{noauthor_notitle_nodate
}

@book{noauthor_notitle_nodate-1
}